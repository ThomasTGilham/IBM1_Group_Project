{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a0e761c-9319-409c-b2fb-1c8381bfffb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\admin\\anaconda3\\lib\\site-packages (2.32.3)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\admin\\anaconda3\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\admin\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: selenium in c:\\users\\admin\\anaconda3\\lib\\site-packages (4.33.0)\n",
      "Requirement already satisfied: webdriver-manager in c:\\users\\admin\\anaconda3\\lib\\site-packages (4.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests) (2025.4.26)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: trio~=0.30.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (0.30.0)\n",
      "Requirement already satisfied: trio-websocket~=0.12.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: typing_extensions~=4.13.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (4.13.2)\n",
      "Requirement already satisfied: websocket-client~=1.8.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\admin\\anaconda3\\lib\\site-packages (from webdriver-manager) (0.21.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\admin\\anaconda3\\lib\\site-packages (from webdriver-manager) (24.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.30.0->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.30.0->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio~=0.30.0->selenium) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from trio-websocket~=0.12.2->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from urllib3[socks]~=2.4.0->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\admin\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.30.0->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4 pandas selenium webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37326a6-2cc5-4b0e-a7a3-f67b87424bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://skillsbuild.org/college-students/course-catalog\"\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Locate the JSON data embedded in the page\n",
    "next_data_script = soup.find(\"script\", id=\"__NEXT_DATA__\")\n",
    "next_data_json = json.loads(next_data_script.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2681e986-d636-49df-8db2-b2efd3b101cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in next_data_json:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd29513f-4842-4d9c-b455-4a85a3a11899",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(next_data_json[\"props\"][\"pageProps\"], indent=2)[:3000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336d48d3-889c-4ba7-980f-92b00a6af512",
   "metadata": {},
   "outputs": [],
   "source": [
    "badges = next_data_json[\"props\"][\"pageProps\"][\"page\"][\"rebrandBadgePageFields\"][\"badgeSection\"]\n",
    "\n",
    "# Extract course info\n",
    "courses = []\n",
    "for section in badges:\n",
    "    for badge in section.get(\"badges\", []):\n",
    "        nodes = badge.get(\"collegeStudentBadge\", {}).get(\"nodes\", [])\n",
    "        for node in nodes:\n",
    "            course = {\n",
    "                \"title\": node.get(\"title\"),\n",
    "                \"slug\": node.get(\"slug\"),\n",
    "                \"description\": node.get(\"rebrandBadgeFields\", {}).get(\"description\"),\n",
    "                \"duration\": node.get(\"rebrandBadgeFields\", {}).get(\"duration\"),\n",
    "                \"link\": node.get(\"rebrandBadgeFields\", {}).get(\"link\"),\n",
    "                \"image\": node.get(\"rebrandBadgeFields\", {}).get(\"image\", {}).get(\"node\", {}).get(\"sourceUrl\"),\n",
    "            }\n",
    "            courses.append(course)\n",
    "\n",
    "# Preview one course\n",
    "print(courses[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0577b4ed-fcb4-4518-8890-3b88f3a69d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Assuming `courses` is your list of course dictionaries from before\n",
    "csv_filename = \"skillsbuild_courses.csv\"\n",
    "\n",
    "# Define the header based on keys in your course dictionaries\n",
    "headers = [\"title\", \"slug\", \"description\", \"duration\", \"link\", \"image\"]\n",
    "\n",
    "# Write to CSV\n",
    "with open(csv_filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=headers)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(courses)\n",
    "\n",
    "print(f\"Courses exported to {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c5916e-a834-4d57-afcd-6e4f7e4cfeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re # Import regex module for more flexible string operations\n",
    "\n",
    "def clean_skillsbuild_courses(input_csv_path=\"skillsbuild_courses.csv\", output_csv_path=\"skillsbuild_courses_cleaned.csv\"):\n",
    "    \"\"\"\n",
    "    Cleans the SkillsBuild course data from a CSV file,\n",
    "    including specific character replacements, duration parsing,\n",
    "    and extraction of duration units.\n",
    "\n",
    "    Args:\n",
    "        input_csv_path (str): Path to the input CSV file.\n",
    "        output_csv_path (str): Path to save the cleaned CSV file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(input_csv_path)\n",
    "        print(f\"Original DataFrame shape: {df.shape}\")\n",
    "        print(\"Original DataFrame head:\")\n",
    "        print(df.head())\n",
    "        print(\"\\nOriginal DataFrame info:\")\n",
    "        df.info()\n",
    "\n",
    "        # --- Cleaning Steps ---\n",
    "\n",
    "        # 1. Handle Missing Values: Fill or drop based on column importance\n",
    "        df['description'] = df['description'].fillna('')\n",
    "        # Keep original duration for unit extraction before numerical conversion\n",
    "        df['original_duration_str'] = df['duration'].fillna('').astype(str) # NEW: Store original string for unit extraction\n",
    "        df['duration'] = df['duration'].fillna('') # Fill for consistency, will be re-processed later\n",
    "        df['link'] = df['link'].fillna('')\n",
    "        df['image'] = df['image'].fillna('')\n",
    "                df.dropna(subset=['title', 'slug'], inplace=True)\n",
    "        print(f\"\\nDataFrame shape after dropping rows with missing 'title' or 'slug': {df.shape}\")\n",
    "\n",
    "        # 2. Remove Duplicates: Based on 'title' and 'link'\n",
    "        df.drop_duplicates(subset=['title', 'link'], inplace=True)\n",
    "        print(f\"DataFrame shape after dropping duplicates: {df.shape}\")\n",
    "\n",
    "        # 3. Clean 'title' and 'description' columns: Remove leading/trailing whitespace\n",
    "        df['title'] = df['title'].str.strip()\n",
    "        df['description'] = df['description'].str.strip()\n",
    "\n",
    "        # --- NEW CLEANING STEPS (Character Replacements) ---\n",
    "\n",
    "        # 4. Replace specific characters in all string columns\n",
    "        def apply_replacements(text):\n",
    "            if isinstance(text, str):\n",
    "                text = text.replace('â„¢', 'TM')\n",
    "                text = text.replace('â€™', \"'\")   # NEW: Added this replacement\n",
    "                text = text.replace('Â®', '®')\n",
    "                text = text.replace('<p>', '')\n",
    "                text = text.replace('</p>', '')\n",
    "                text = text.replace('&#8217;', \"'\")\n",
    "            return text\n",
    "\n",
    "        # Apply the replacements to all relevant string columns\n",
    "        # Note: 'original_duration_str' is also a string, so it gets cleaned too.\n",
    "        for col in ['title', 'slug', 'description', 'original_duration_str', 'link', 'image']:\n",
    "            if col in df.columns and df[col].dtype == 'object':\n",
    "                df[col] = df[col].apply(apply_replacements)\n",
    "        print(\"\\nApplied specific character and HTML tag replacements.\")\n",
    "\n",
    "        # --- NEW COLUMN: unit_duration ---\n",
    "        df['unit_duration'] = None # Initialize new column\n",
    "\n",
    "        def extract_duration_unit(duration_str):\n",
    "            duration_str_lower = str(duration_str).lower()\n",
    "            if 'minute' in duration_str_lower:\n",
    "                return 'minutes'\n",
    "            elif 'hour' in duration_str_lower:\n",
    "                return 'hours'\n",
    "            elif 'day' in duration_str_lower:\n",
    "                return 'days'\n",
    "            elif 'week' in duration_str_lower:\n",
    "                return 'weeks'\n",
    "            elif 'month' in duration_str_lower:\n",
    "                return 'months'\n",
    "            elif 'year' in duration_str_lower:\n",
    "                return 'years'\n",
    "            # Add more specific unit checks if necessary\n",
    "            # For general cases, you might extract any word next to a number\n",
    "            match = re.search(r'\\d+\\s*([a-zA-Z]+)', duration_str_lower)\n",
    "            if match:\n",
    "                unit = match.group(1)\n",
    "                # You might want to normalize units (e.g., \"hrs\" to \"hours\")\n",
    "                if unit.startswith('hr'): return 'hours'\n",
    "                if unit.startswith('min'): return 'minutes'\n",
    "                if unit.startswith('wk'): return 'weeks'\n",
    "                if unit.startswith('day'): return 'days'\n",
    "                if unit.startswith('mo'): return 'months'\n",
    "                if unit.startswith('yr'): return 'years'\n",
    "                return unit # Return the extracted unit if not specifically mapped\n",
    "            return None # Default if no unit found\n",
    "        # Apply unit extraction to the stored original duration string\n",
    "        df['unit_duration'] = df['original_duration_str'].apply(extract_duration_unit)\n",
    "        print(\"\\nCreated 'unit_duration' column.\")\n",
    "\n",
    "        # 5. Clean 'duration' column: Only keep the first 3 digits and make sure they are numbers\n",
    "        # Use the stored original string for this, as it's cleaner than the partially filled 'duration'\n",
    "        df['duration'] = df['original_duration_str'].astype(str) # Re-assign original string to 'duration' for processing\n",
    "\n",
    "        def extract_duration_digits(duration_str):\n",
    "            numbers = re.findall(r'\\d+', duration_str)\n",
    "            if numbers:\n",
    "                first_num = numbers[0]\n",
    "                cleaned_num_str = first_num[:3]\n",
    "                try:\n",
    "                    return int(cleaned_num_str)\n",
    "                except ValueError:\n",
    "                    return None\n",
    "            return None\n",
    "\n",
    "        df['duration'] = df['duration'].apply(extract_duration_digits)\n",
    "        df['duration'] = df['duration'].fillna(0) # Fill with 0 for missing/unparseable numeric durations\n",
    "        df['duration'] = pd.to_numeric(df['duration'], errors='coerce').astype('Int64') # Ensure integer type\n",
    "        print(\"\\nCleaned 'duration' column to keep only first 3 digits and convert to number.\")\n",
    "\n",
    "        # 6. Validate 'link' format\n",
    "        df['link'] = df['link'].astype(str).apply(lambda x: x if x.startswith(('http://', 'https://')) else '')\n",
    "\n",
    "        # 7. Remove rows where 'title' is empty after stripping\n",
    "        df = df[df['title'] != '']\n",
    "        print(f\"DataFrame shape after removing rows with empty 'title': {df.shape}\")\n",
    "       # --- Remove the temporary 'original_duration_str' column ---\n",
    "        df.drop(columns=['original_duration_str'], inplace=True)\n",
    "        print(\"\\nDropped temporary 'original_duration_str' column.\")\n",
    "\n",
    "        # --- Save the Cleaned Data ---\n",
    "        df.to_csv(output_csv_path, index=False, encoding='utf-8')\n",
    "        print(f\"\\nCleaned data exported to {output_csv_path}\")\n",
    "        print(\"\\nCleaned DataFrame head:\")\n",
    "        print(df.head())\n",
    "        print(\"\\nCleaned DataFrame info:\")\n",
    "        df.info()\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{input_csv_path}' was not found. Please ensure the CSV file exists.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during cleaning: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736d9341-6e2e-40f1-a2c8-d71c14231f23",
   "metadata": {},
   "source": [
    "**Data Science Courses**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68d89e55-d3ac-42b7-9c5a-f052af815ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: webdriver-manager in c:\\users\\admin\\anaconda3\\lib\\site-packages (4.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\anaconda3\\lib\\site-packages (from webdriver-manager) (2.32.3)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\admin\\anaconda3\\lib\\site-packages (from webdriver-manager) (0.21.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\admin\\anaconda3\\lib\\site-packages (from webdriver-manager) (24.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2025.4.26)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cecec14c-1d89-48f4-9930-dfc2273ef4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\admin\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\admin\\anaconda3\\lib\\site-packages (3.1.5)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\admin\\anaconda3\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c8acc23-ea5b-4338-83d5-a99c92fad7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Navigating to initial IBM SkillsBuild login page...\n",
      "Attempting to handle cookie consent banner...\n",
      "Clicked 'Accept all' on cookie banner.\n",
      "Attempting to click 'Log in with Email' button...\n",
      "Clicked 'Log in with Email' button.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter your email:  zo24176@bristol.ac.uk\n",
      "Please enter your password:  IBM24176Bristol$\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to find and fill email field...\n",
      "Email entered.\n",
      "Attempting to find and fill password field...\n",
      "Password entered.\n",
      "Attempting to find and click submit button...\n",
      "Submit button clicked.\n",
      "Login process complete. Proceeding to target pages.\n",
      "\n",
      "--- Scraping Course 1 of 4: https://skills.yourlearning.ibm.com/activity/PLAN-14F2691E3A32?ngo-id=0302 ---\n",
      "Attempting to handle cookie consent banner...\n",
      "Clicked 'Accept all' on cookie banner.\n",
      "Activity page loaded successfully.\n",
      "Scraped Title: Getting Started with Data (Earn a credential!)\n",
      "Scraped Overall Duration: About 3 hours\n",
      "Scraped Overall Learners: 5860 learners have completed this activity in the past 12 months. (Numeric: 5860)\n",
      "Scraped Overall Rating (Header): Average rating of 4.5 stars by 176 learners in the past 12 months.\n",
      "Scraped 'About' description: Are you curious to discover the type of insights we can extract from data? Organizations use data to...\n",
      "\n",
      "--- Attempting to scrape Courses section ---\n",
      "Waiting for courses section parent 'learningPlanSectionSECTION-B' to be present...\n",
      "Found courses section parent.\n",
      "Scrolled entire 'Courses' section parent into view (top alignment).\n",
      "Attempting to find 'Courses' accordion button...\n",
      "Found 'Courses' accordion button.\n",
      "'Courses' accordion is already expanded.\n",
      "Attempting to find 'Complete X required' text...\n",
      "Scraped Total Courses Text: Complete 3 required\n",
      "Waiting for course cards to be visible...\n",
      "Found 3 course elements.\n",
      "  - Course 1: Name='Introduction to Data Concepts', Duration='1 hr 15 mins'...\n",
      "  - Course 2: Name='Data Science Landscape', Duration='1 hr'...\n",
      "  - Course 3: Name='Introduction to Tableau Desktop', Duration='1 hr'...\n",
      "\n",
      "--- Attempting to scrape Tags section ---\n",
      "Scrolled Tags section into view.\n",
      "Scraped Tags: ['Administrative', 'NotSearchable', 'Learning Domain', 'Technical skills - Data', 'Learning Domain Primary', 'Data']\n",
      "\n",
      "--- Attempting to scrape Ratings & Reviews section ---\n",
      "Scrolled Ratings & Reviews section into view.\n",
      "Scraped Overall Average Rating: 4.5\n",
      "Scraped Number of Learners Rated: Average rating by 176 learners\n",
      "Scraped Rating Breakdown: {}\n",
      "\n",
      "--- Attempting to scrape individual comments ---\n",
      "Found comments list container.\n",
      "No more 'Load more comments' button found or all comments loaded.\n",
      "Found 10 individual comments for https://skills.yourlearning.ibm.com/activity/PLAN-14F2691E3A32?ngo-id=0302.\n",
      "  - Comment 1 by Rahul Gupta: Rating='5', Date='14 Jun 2025'...\n",
      "  - Comment 2 by Annam Shireesha: Rating='5', Date='13 Jun 2025'...\n",
      "  - Comment 3 by B vijay kumar Reddy: Rating='1', Date='13 Jun 2025'...\n",
      "  - Comment 4 by Raavi Aparna: Rating='N/A', Date='10 Jun 2025'...\n",
      "  - Comment 5 by Dinakar Bethapudi: Rating='4', Date='10 Jun 2025'...\n",
      "  - Comment 6 by HODIAM HAGIDI DINESH KUMAR: Rating='5', Date='09 Jun 2025'...\n",
      "  - Comment 7 by Naga Akhila Telasula: Rating='5', Date='23 May 2025'...\n",
      "  - Comment 8 by BHIMAVARAPU VENKATA PAVAN: Rating='5', Date='09 Jun 2025'...\n",
      "  - Comment 9 by PRABHAT RECB: Rating='5', Date='01 Jun 2025'...\n",
      "  - Comment 10 by siddamcharankuma reddy: Rating='5', Date='29 May 2025'...\n",
      "\n",
      "--- Finished scraping for Getting Started with Data (Earn a credential!) ---\n",
      "\n",
      "--- Scraping Course 2 of 4: https://skills.yourlearning.ibm.com/activity/PLAN-BC0FAEE8E439?ngo-id=0302 ---\n",
      "Attempting to handle cookie consent banner...\n",
      "No 'Accept all' button found or it was not clickable within timeout.\n",
      "No 'Required only' button found or it was not clickable within timeout.\n",
      "Cookie banner not handled (may not be present or different structure).\n",
      "Activity page loaded successfully.\n",
      "Scraped Title: Data Fundamentals (Earn a credential!)\n",
      "Scraped Overall Duration: About 7 hours\n",
      "Scraped Overall Learners: 40262 learners have completed this activity in the past 12 months. (Numeric: 40262)\n",
      "Scraped Overall Rating (Header): Average rating of 4.5 stars by 1235 learners in the past 12 months.\n",
      "Scraped 'About' description: Do you love to discover meaning in facts and numbers? Learn the concepts and methods of data science...\n",
      "\n",
      "--- Attempting to scrape Courses section ---\n",
      "Waiting for courses section parent 'learningPlanSectionSECTION-B' to be present...\n",
      "Found courses section parent.\n",
      "Scrolled entire 'Courses' section parent into view (top alignment).\n",
      "Attempting to find 'Courses' accordion button...\n",
      "Found 'Courses' accordion button.\n",
      "'Courses' accordion is already expanded.\n",
      "Attempting to find 'Complete X required' text...\n",
      "Scraped Total Courses Text: Complete 5 required\n",
      "Waiting for course cards to be visible...\n",
      "Found 5 course elements.\n",
      "  - Course 1: Name='Introduction to Data Concepts', Duration='1 hr 15 mins'...\n",
      "  - Course 2: Name='Data Science in Our World', Duration='2 hrs'...\n",
      "  - Course 3: Name='Overview of Data Tools and Languages', Duration='1 hr 30 mins'...\n",
      "  - Course 4: Name='Clean, Refine, and Visualize Data with IBM Watson Studio', Duration='1 hr 35 mins'...\n",
      "  - Course 5: Name='Your Future in Data: The Job Landscape', Duration='50 mins'...\n",
      "\n",
      "--- Attempting to scrape Tags section ---\n",
      "Scrolled Tags section into view.\n",
      "Scraped Tags: ['Badge Learning', '000 All digital credentials', 'Data Fundamentals', 'Badge Learning - Academia', '000 All digital credentials - Academia', 'Fundamentals', 'All Fundamentals', 'Learning Domain', 'Technical skills - Data', 'Learning Domain Primary', 'Data', 'Lightcast Skills Taxonomy', 'Data Analysis Skill', 'Data Science Skill']\n",
      "\n",
      "--- Attempting to scrape Ratings & Reviews section ---\n",
      "Scrolled Ratings & Reviews section into view.\n",
      "Scraped Overall Average Rating: 4.5\n",
      "Scraped Number of Learners Rated: Average rating by 1,235 learners\n",
      "Scraped Rating Breakdown: {}\n",
      "\n",
      "--- Attempting to scrape individual comments ---\n",
      "Found comments list container.\n",
      "No more 'Load more comments' button found or all comments loaded.\n",
      "Found 11 individual comments for https://skills.yourlearning.ibm.com/activity/PLAN-BC0FAEE8E439?ngo-id=0302.\n",
      "  - Comment 1 by ROHIT RENJI GEORGE Btech-CSE 2K22: Rating='5', Date='16 Jun 2025'...\n",
      "  - Comment 2 by Fagih Nabeel: Rating='1', Date='15 Jun 2025'...\n",
      "  - Comment 3 by Mfundo Mhlabeni: Rating='5', Date='14 Jun 2025'...\n",
      "  - Comment 4 by Emily Ghukasyan: Rating='5', Date='13 Jun 2025'...\n",
      "  - Comment 5 by Guilherme Ferraz: Rating='2', Date='12 Jun 2025'...\n",
      "  - Comment 6 by Juan Jose Aguilera Ramos: Rating='N/A', Date='13 Jun 2025'...\n",
      "  - Comment 7 by NELSON ARTURO ALARCON LAURA: Rating='N/A', Date='12 Jun 2025'...\n",
      "  - Comment 8 by WALEED ALAMAL: Rating='5', Date='07 Jun 2025'...\n",
      "  - Comment 9 by Osama Mohamed Hassan: Rating='5', Date='10 Jun 2025'...\n",
      "  - Comment 10 by Arezou razeghian: Rating='5', Date='08 Jun 2025'...\n",
      "  - Comment 11 by Rangesh Pandian PT: Rating='5', Date='08 Jun 2025'...\n",
      "\n",
      "--- Finished scraping for Data Fundamentals (Earn a credential!) ---\n",
      "\n",
      "--- Scraping Course 3 of 4: https://skills.yourlearning.ibm.com/activity/PLAN-0D62D9A52C35?ngo-id=0302&_gl=1*rcdm7y*_ga*NjUyOTg1MDUyLjE3NDkxMjA0MTI.*_ga_FYECCCS21D*czE3NDkxMjc2ODMkbzIkZzAkdDE3NDkxMjc2ODMkajYwJGwwJGgw ---\n",
      "Attempting to handle cookie consent banner...\n",
      "No 'Accept all' button found or it was not clickable within timeout.\n",
      "No 'Required only' button found or it was not clickable within timeout.\n",
      "Cookie banner not handled (may not be present or different structure).\n",
      "Activity page loaded successfully.\n",
      "Scraped Title: Enterprise Data Science in Practice\n",
      "Scraped Overall Duration: About 10 hours\n",
      "Scraped Overall Learners: 1179 learners have completed this activity in the past 12 months. (Numeric: 1179)\n",
      "Scraped Overall Rating (Header): Average rating of 4.5 stars by 18 learners in the past 12 months.\n",
      "Scraped 'About' description: This course content is intended to assist individuals with an active interest in gaining a better po...\n",
      "\n",
      "--- Attempting to scrape Courses section ---\n",
      "Waiting for courses section parent 'learningPlanSectionSECTION-B' to be present...\n",
      "Found courses section parent.\n",
      "Scrolled entire 'Courses' section parent into view (top alignment).\n",
      "Attempting to find 'Courses' accordion button...\n",
      "Found 'Courses' accordion button.\n",
      "'Courses' accordion is already expanded.\n",
      "Attempting to find 'Complete X required' text...\n",
      "Scraped Total Courses Text: Complete 1 required\n",
      "Waiting for course cards to be visible...\n",
      "Found 1 course elements.\n",
      "  - Course 1: Name='Enterprise Data Science in Practice', Duration='10 hrs'...\n",
      "\n",
      "--- Attempting to scrape Tags section ---\n",
      "Scrolled Tags section into view.\n",
      "Scraped Tags: ['Administrative', 'NotSearchable', 'Badge Learning', '000 All digital credentials', 'Badge Learning - Academia', '000 All digital credentials - Academia', 'Learning Domain Primary', 'Data']\n",
      "\n",
      "--- Attempting to scrape Ratings & Reviews section ---\n",
      "Scrolled Ratings & Reviews section into view.\n",
      "Scraped Overall Average Rating: 4.5\n",
      "Scraped Number of Learners Rated: Average rating by 18 learners\n",
      "Scraped Rating Breakdown: {}\n",
      "\n",
      "--- Attempting to scrape individual comments ---\n",
      "Found comments list container.\n",
      "No more 'Load more comments' button found or all comments loaded.\n",
      "Found 3 individual comments for https://skills.yourlearning.ibm.com/activity/PLAN-0D62D9A52C35?ngo-id=0302&_gl=1*rcdm7y*_ga*NjUyOTg1MDUyLjE3NDkxMjA0MTI.*_ga_FYECCCS21D*czE3NDkxMjc2ODMkbzIkZzAkdDE3NDkxMjc2ODMkajYwJGwwJGgw.\n",
      "  - Comment 1 by João Heguedusch: Rating='5', Date='28 Apr 2025'...\n",
      "  - Comment 2 by ROBERT GUERRERO: Rating='N/A', Date='13 Nov 2024'...\n",
      "  - Comment 3 by Keerthi Soma: Rating='3', Date='20 Sep 2024'...\n",
      "\n",
      "--- Finished scraping for Enterprise Data Science in Practice ---\n",
      "\n",
      "--- Scraping Course 4 of 4: https://skills.yourlearning.ibm.com/activity/PLAN-D8E7C82C1D76?ngo-id=0302&_gl=1*1ijk7w0*_ga*NjUyOTg1MDUyLjE3NDkxMjA0MTI.*_ga_FYECCCS21D*czE3NDkxMjc2ODMkbzIkZzAkdDE3NDkxMjc2ODQkajU5JGwwJGgw ---\n",
      "Attempting to handle cookie consent banner...\n",
      "No 'Accept all' button found or it was not clickable within timeout.\n",
      "No 'Required only' button found or it was not clickable within timeout.\n",
      "Cookie banner not handled (may not be present or different structure).\n",
      "Activity page loaded successfully.\n",
      "Scraped Title: Machine Learning for Data Science Projects\n",
      "Scraped Overall Duration: About 20 hours\n",
      "Scraped Overall Learners: 914 learners have completed this activity in the past 12 months. (Numeric: 914)\n",
      "Scraped Overall Rating (Header): Average rating of 4.5 stars by 18 learners in the past 12 months.\n",
      "Scraped 'About' description: This course content is intended to assist individuals with an active interest in gaining a better po...\n",
      "\n",
      "--- Attempting to scrape Courses section ---\n",
      "Waiting for courses section parent 'learningPlanSectionSECTION-B' to be present...\n",
      "Found courses section parent.\n",
      "Scrolled entire 'Courses' section parent into view (top alignment).\n",
      "Attempting to find 'Courses' accordion button...\n",
      "Found 'Courses' accordion button.\n",
      "'Courses' accordion is already expanded.\n",
      "Attempting to find 'Complete X required' text...\n",
      "Scraped Total Courses Text: Complete 1 required\n",
      "Waiting for course cards to be visible...\n",
      "Found 1 course elements.\n",
      "  - Course 1: Name='Machine Learning for Data Science Projects', Duration='20 hrs'...\n",
      "\n",
      "--- Attempting to scrape Tags section ---\n",
      "Scrolled Tags section into view.\n",
      "Scraped Tags: ['Administrative', 'NotSearchable', 'Badge Learning', '000 All digital credentials', 'Badge Learning - Academia', '000 All digital credentials - Academia', 'Learning Domain', 'Technical skills - Data', 'Learning Domain Primary', 'Data']\n",
      "\n",
      "--- Attempting to scrape Ratings & Reviews section ---\n",
      "Scrolled Ratings & Reviews section into view.\n",
      "Scraped Overall Average Rating: 4.5\n",
      "Scraped Number of Learners Rated: Average rating by 18 learners\n",
      "Scraped Rating Breakdown: {}\n",
      "\n",
      "--- Attempting to scrape individual comments ---\n",
      "Found comments list container.\n",
      "No more 'Load more comments' button found or all comments loaded.\n",
      "Found 6 individual comments for https://skills.yourlearning.ibm.com/activity/PLAN-D8E7C82C1D76?ngo-id=0302&_gl=1*1ijk7w0*_ga*NjUyOTg1MDUyLjE3NDkxMjA0MTI.*_ga_FYECCCS21D*czE3NDkxMjc2ODMkbzIkZzAkdDE3NDkxMjc2ODQkajU5JGwwJGgw.\n",
      "  - Comment 1 by JANAK BHANDARI: Rating='5', Date='09 Apr 2025'...\n",
      "  - Comment 2 by Koushalya Kousha: Rating='4', Date='20 Mar 2025'...\n",
      "  - Comment 3 by Teja Mallam: Rating='N/A', Date='22 Feb 2025'...\n",
      "  - Comment 4 by Amal Saleh Alamro: Rating='4', Date='17 Feb 2025'...\n",
      "  - Comment 5 by Ahmed Mazghouni: Rating='5', Date='05 Jan 2025'...\n",
      "  - Comment 6 by Sabin Gautam: Rating='N/A', Date='08 Dec 2024'...\n",
      "\n",
      "--- Finished scraping for Machine Learning for Data Science Projects ---\n",
      "\n",
      "--- Consolidating and Saving All Scraped Data ---\n",
      "\n",
      "Saved 'ds_general_info.csv'\n",
      "Saved 'ds_courses.csv'\n",
      "Saved 'ds_comments.csv'\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException, StaleElementReferenceException\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# Set up Chrome options\n",
    "options = Options()\n",
    "\n",
    "# Use ChromeDriverManager to automatically handle the ChromeDriver executable\n",
    "s = Service(ChromeDriverManager().install())\n",
    "\n",
    "# Initialize the Chrome driver (only once)\n",
    "driver = webdriver.Chrome(service=s, options=options)\n",
    "\n",
    "# Add a WebDriverWait instance\n",
    "wait = WebDriverWait(driver, 30)\n",
    "\n",
    "def handle_cookie_consent(driver, wait):\n",
    "    \"\"\"\n",
    "    Attempts to find and click an 'Accept all' or 'Required only' button on a cookie consent banner.\n",
    "    \"\"\"\n",
    "    print(\"Attempting to handle cookie consent banner...\")\n",
    "    try:\n",
    "        accept_all_button_locator = (By.XPATH, \"//button[contains(translate(., 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz'), 'accept all')]\")\n",
    "        accept_all_button = wait.until(EC.element_to_be_clickable(accept_all_button_locator))\n",
    "        accept_all_button.click()\n",
    "        print(\"Clicked 'Accept all' on cookie banner.\")\n",
    "        time.sleep(2)\n",
    "        return True\n",
    "    except TimeoutException:\n",
    "        print(\"No 'Accept all' button found or it was not clickable within timeout.\")\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        print(f\"Error clicking 'Accept all' button: {e}\")\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        required_only_button_locator = (By.XPATH, \"//button[contains(translate(., 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz'), 'required only')]\")\n",
    "        required_only_button = wait.until(EC.element_to_be_clickable(required_only_button_locator))\n",
    "        required_only_button.click()\n",
    "        print(\"Clicked 'Required only' on cookie banner.\")\n",
    "        time.sleep(2)\n",
    "        return True\n",
    "    except TimeoutException:\n",
    "        print(\"No 'Required only' button found or it was not clickable within timeout.\")\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        print(f\"Error clicking 'Required only' button: {e}\")\n",
    "        pass\n",
    "\n",
    "    print(\"Cookie banner not handled (may not be present or different structure).\")\n",
    "    return False\n",
    "\n",
    "# --- Login Process ---\n",
    "# This part remains the same as you need to log in only once before accessing any course pages.\n",
    "driver.get('http://www.google.com/')\n",
    "time.sleep(1)\n",
    "\n",
    "print(\"Navigating to initial IBM SkillsBuild login page...\")\n",
    "driver.get(\"https://sb-auth.skillsbuild.org/login?ngo-id=0302\")\n",
    "time.sleep(3)\n",
    "handle_cookie_consent(driver, wait)\n",
    "\n",
    "print(\"Attempting to click 'Log in with Email' button...\")\n",
    "try:\n",
    "    email_login_button_locator = (By.XPATH, \"//a[@data-attribute1='Log in with Email']\")\n",
    "    email_login_button = wait.until(EC.element_to_be_clickable(email_login_button_locator))\n",
    "    email_login_button.click()\n",
    "    print(\"Clicked 'Log in with Email' button.\")\n",
    "    time.sleep(5)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to click 'Log in with Email' button: {e}\")\n",
    "    driver.quit()\n",
    "    exit()\n",
    "\n",
    "user_email = input(\"Please enter your email: \")\n",
    "user_password = input(\"Please enter your password: \")\n",
    "\n",
    "try:\n",
    "    print(\"Attempting to find and fill email field...\")\n",
    "    email_field = wait.until(EC.presence_of_element_located((By.ID, \"ibm-label-0\")))\n",
    "    email_field.send_keys(user_email)\n",
    "    print(\"Email entered.\")\n",
    "\n",
    "    print(\"Attempting to find and fill password field...\")\n",
    "    password_field = wait.until(EC.presence_of_element_located((By.ID, \"ibm-label-1\")))\n",
    "    password_field.send_keys(user_password)\n",
    "    print(\"Password entered.\")\n",
    "\n",
    "    print(\"Attempting to find and click submit button...\")\n",
    "    submit_button = wait.until(EC.element_to_be_clickable((By.XPATH, \"//button[@type='submit']\")))\n",
    "    submit_button.click()\n",
    "    print(\"Submit button clicked.\")\n",
    "\n",
    "    time.sleep(5)\n",
    "    print(\"Login process complete. Proceeding to target pages.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during login form submission: {e}\")\n",
    "    driver.quit()\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- List of Target URLs ---\n",
    "target_urls = [\n",
    "    \"https://skills.yourlearning.ibm.com/activity/PLAN-14F2691E3A32?ngo-id=0302\", # Getting Started with Data\n",
    "    \"https://skills.yourlearning.ibm.com/activity/PLAN-BC0FAEE8E439?ngo-id=0302\", # Data Science Foundations\n",
    "    \"https://skills.yourlearning.ibm.com/activity/PLAN-0D62D9A52C35?ngo-id=0302&_gl=1*rcdm7y*_ga*NjUyOTg1MDUyLjE3NDkxMjA0MTI.*_ga_FYECCCS21D*czE3NDkxMjc2ODMkbzIkZzAkdDE3NDkxMjc2ODMkajYwJGwwJGgw\", # Data Analytics\n",
    "    \"https://skills.yourlearning.ibm.com/activity/PLAN-D8E7C82C1D76?ngo-id=0302&_gl=1*1ijk7w0*_ga*NjUyOTg1MDUyLjE3NDkxMjA0MTI.*_ga_FYECCCS21D*czE3NDkxMjc2ODMkbzIkZzAkdDE3NDkxMjc2ODQkajU5JGwwJGgw\"  # AI Foundations\n",
    "]\n",
    "\n",
    "# --- Lists to store data from all courses ---\n",
    "all_general_info_data = []\n",
    "all_courses_data = []\n",
    "all_comments_data = []\n",
    "all_rating_breakdown_data = []\n",
    "\n",
    "# --- Loop through each URL and scrape ---\n",
    "for i, url in enumerate(target_urls):\n",
    "    print(f\"\\n--- Scraping Course {i+1} of {len(target_urls)}: {url} ---\")\n",
    "    driver.get(url)\n",
    "    handle_cookie_consent(driver, wait) # Re-check for cookies on new page load\n",
    "\n",
    "    scraped_data = {} # Reset scraped_data for each URL\n",
    "    \n",
    "    # Add a unique identifier for each course, e.g., its URL or title\n",
    "    scraped_data['course_url'] = url\n",
    "\n",
    "    try:\n",
    "        # Wait for a prominent element on the page to ensure it's loaded\n",
    "        wait.until(EC.presence_of_element_located((By.XPATH, \"//h1[contains(@class, 'FullPageHeader_fullPageHeader__title__')]/span\")))\n",
    "        print(\"Activity page loaded successfully.\")\n",
    "        time.sleep(5) # Give it a bit more time for dynamic content\n",
    "    except TimeoutException:\n",
    "        print(f\"Timed out waiting for activity page {url} to load. Skipping this URL.\")\n",
    "        continue # Skip to the next URL\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while loading page {url}: {e}. Skipping this URL.\")\n",
    "        continue\n",
    "\n",
    "    # --- Start scraping individual elements for the current URL ---\n",
    "\n",
    "    # 1. Scrape Learning Plan Title\n",
    "    try:\n",
    "        title_element = driver.find_element(By.XPATH, \"//h1[contains(@class, 'FullPageHeader_fullPageHeader__title__')]/span\")\n",
    "        scraped_data['learning_plan_title'] = title_element.text.strip()\n",
    "        print(f\"Scraped Title: {scraped_data['learning_plan_title']}\")\n",
    "    except NoSuchElementException:\n",
    "        print(\"Could not find Learning Plan Title.\")\n",
    "        scraped_data['learning_plan_title'] = \"N/A\"\n",
    "\n",
    "    # 2. Scrape overall duration, learners amount, and overall rating from header\n",
    "    try:\n",
    "        header_info_div = driver.find_element(By.XPATH, \"//div[contains(@class, 'FullPageHeader_fullPageHeader__info__')]\")\n",
    "        try:\n",
    "            duration_element = header_info_div.find_element(By.XPATH, \".//div[contains(@class, 'Time_container__')]/div[contains(@id, 'a11y-undefined-time')]/span[not(contains(@class, 'sr-only'))]\")\n",
    "            scraped_data['overall_duration'] = duration_element.text.strip()\n",
    "        except NoSuchElementException:\n",
    "            scraped_data['overall_duration'] = \"N/A\"\n",
    "\n",
    "        try:\n",
    "            learners_element = header_info_div.find_element(By.XPATH, \".//div[contains(@class, 'LearnersAmount_learnersAmount__')]\")\n",
    "            learners_title = learners_element.get_attribute('title')\n",
    "            scraped_data['overall_learners_amount_text'] = learners_title.strip() if learners_title else \"N/A\"\n",
    "            match = re.search(r'(\\d[\\d,\\.]*) learners', scraped_data['overall_learners_amount_text'])\n",
    "            if match:\n",
    "                scraped_data['overall_learners_amount_numeric'] = match.group(1).replace('.', '').replace(',', '')\n",
    "            else:\n",
    "                scraped_data['overall_learners_amount_numeric'] = \"N/A\"\n",
    "        except NoSuchElementException:\n",
    "            scraped_data['overall_learners_amount_text'] = \"N/A\"\n",
    "            scraped_data['overall_learners_amount_numeric'] = \"N/A\"\n",
    "\n",
    "        try:\n",
    "            overall_rating_element = header_info_div.find_element(By.XPATH, \".//div[contains(@class, 'Stars_starRating__C3-hw')]\")\n",
    "            overall_rating_title = overall_rating_element.get_attribute('title')\n",
    "            scraped_data['overall_rating_header_text'] = overall_rating_title.strip() if overall_rating_title else \"N/A\"\n",
    "            rating_match = re.search(r'Average rating of ([\\d\\.]+) stars by (\\d+) learners', scraped_data['overall_rating_header_text'])\n",
    "            if rating_match:\n",
    "                scraped_data['overall_average_rating_from_header'] = rating_match.group(1)\n",
    "                scraped_data['overall_learners_rated_from_header'] = rating_match.group(2)\n",
    "            else:\n",
    "                scraped_data['overall_average_rating_from_header'] = \"N/A\"\n",
    "                scraped_data['overall_learners_rated_from_header'] = \"N/A\"\n",
    "        except NoSuchElementException:\n",
    "            scraped_data['overall_rating_header_text'] = \"N/A\"\n",
    "            scraped_data['overall_average_rating_from_header'] = \"N/A\"\n",
    "            scraped_data['overall_learners_rated_from_header'] = \"N/A\"\n",
    "\n",
    "        print(f\"Scraped Overall Duration: {scraped_data.get('overall_duration')}\")\n",
    "        print(f\"Scraped Overall Learners: {scraped_data.get('overall_learners_amount_text')} (Numeric: {scraped_data.get('overall_learners_amount_numeric')})\")\n",
    "        print(f\"Scraped Overall Rating (Header): {scraped_data.get('overall_rating_header_text')}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping header info: {e}\")\n",
    "        scraped_data['overall_duration'] = \"N/A\"\n",
    "        scraped_data['overall_learners_amount_text'] = \"N/A\"\n",
    "        scraped_data['overall_learners_amount_numeric'] = \"N/A\"\n",
    "        scraped_data['overall_rating_header_text'] = \"N/A\"\n",
    "        scraped_data['overall_average_rating_from_header'] = \"N/A\"\n",
    "        scraped_data['overall_learners_rated_from_header'] = \"N/A\"\n",
    "\n",
    "    # 3. Scrape \"About this learning plan\" description\n",
    "    try:\n",
    "        description_content_div = wait.until(EC.presence_of_element_located(\n",
    "            (By.XPATH, \"//div[contains(@class, 'FullPageDescription_wrapper__')]/div[contains(@class, 'FullPageDescription_content__')]\")\n",
    "        ))\n",
    "        scraped_data['about_learning_plan'] = description_content_div.text.strip()\n",
    "        print(f\"Scraped 'About' description: {scraped_data['about_learning_plan'][:100]}...\")\n",
    "    except NoSuchElementException:\n",
    "        print(\"Could not find 'About this learning plan' description content.\")\n",
    "        scraped_data['about_learning_plan'] = \"N/A\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping 'About this learning plan' description: {e}\")\n",
    "        scraped_data['about_learning_plan'] = \"N/A\"\n",
    "\n",
    "    # 4. Scrape Courses and their details\n",
    "    current_course_list = [] # Store courses for the current URL\n",
    "    try:\n",
    "        print(\"\\n--- Attempting to scrape Courses section ---\")\n",
    "        courses_section_parent_locator = (By.ID, \"learningPlanSectionSECTION-B\")\n",
    "        \n",
    "        # New: More direct locator for the button based on the HTML you provided\n",
    "        # The button has role=\"button\" implicitly or explicitly, and contains the title \"Course: ...\"\n",
    "        # Let's try to find it by its title or direct relation to the parent li\n",
    "        courses_accordion_button_locator = (By.XPATH, \n",
    "            \"//div[@id='learningPlanSectionSECTION-B']//li[contains(@class, 'bx-yl--accordion__item')]//button[contains(@class, 'bx-yl--accordion__heading')]\"\n",
    "        )\n",
    "\n",
    "        courses_section_found_and_handled = False \n",
    "\n",
    "        try:\n",
    "            print(f\"Waiting for courses section parent '{courses_section_parent_locator[1]}' to be present...\")\n",
    "            courses_section_parent = wait.until(EC.presence_of_element_located(courses_section_parent_locator))\n",
    "            print(\"Found courses section parent.\")\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView(true);\", courses_section_parent)\n",
    "            print(\"Scrolled entire 'Courses' section parent into view (top alignment).\")\n",
    "            time.sleep(3)\n",
    "\n",
    "            print(\"Attempting to find 'Courses' accordion button...\")\n",
    "            courses_accordion_button = wait.until(EC.presence_of_element_located(courses_accordion_button_locator))\n",
    "            print(\"Found 'Courses' accordion button.\")\n",
    "            \n",
    "            # Check aria-expanded state directly\n",
    "            if courses_accordion_button.get_attribute(\"aria-expanded\") == \"false\":\n",
    "                print(\"Accordion is collapsed, attempting to click to expand.\")\n",
    "                # Ensure it's clickable before clicking\n",
    "                wait.until(EC.element_to_be_clickable(courses_accordion_button_locator)).click()\n",
    "                print(\"Clicked to expand 'Courses' accordion.\")\n",
    "                time.sleep(5)\n",
    "            else:\n",
    "                print(\"'Courses' accordion is already expanded.\")\n",
    "                time.sleep(2)\n",
    "            \n",
    "            courses_section_found_and_handled = True\n",
    "\n",
    "        except TimeoutException:\n",
    "            print(\"Timeout: 'Courses' accordion button not found within timeout.\")\n",
    "            # Even if the button isn't clickable, the content might still be there if it's already expanded\n",
    "            # So, we don't necessarily set courses_section_found_and_handled to False here yet.\n",
    "            # We will try to find content anyway.\n",
    "            pass # Keep it False for now if button isn't clickable\n",
    "        except StaleElementReferenceException:\n",
    "            print(\"Stale element reference for 'Courses' accordion button. Retrying to find.\")\n",
    "            try:\n",
    "                courses_accordion_button = wait.until(EC.presence_of_element_located(courses_accordion_button_locator))\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView(true);\", courses_accordion_button)\n",
    "                time.sleep(3)\n",
    "                if courses_accordion_button.get_attribute(\"aria-expanded\") == \"false\":\n",
    "                    wait.until(EC.element_to_be_clickable(courses_accordion_button_locator)).click()\n",
    "                    print(\"Clicked to expand 'Courses' accordion after stale error.\")\n",
    "                    time.sleep(5)\n",
    "                courses_section_found_and_handled = True\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to handle stale element for Courses accordion on retry: {e}\")\n",
    "                # If retry also fails, then it truly wasn't handled.\n",
    "                scraped_data['courses_section_status'] = \"Failed to handle stale accordion button\"\n",
    "                courses_section_found_and_handled = False # Explicitly set to False here\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred while handling Courses accordion button: {e}\")\n",
    "            scraped_data['courses_section_status'] = \"Error during accordion button handling\"\n",
    "            courses_section_found_and_handled = False # Explicitly set to False here\n",
    "\n",
    "        # Regardless of whether the accordion was clicked or not, if the parent section was found,\n",
    "        # we should attempt to scrape the course cards, as they might be visible by default.\n",
    "        if courses_section_parent: # Check if the parent div was found\n",
    "            try:\n",
    "                print(\"Attempting to find 'Complete X required' text...\")\n",
    "                # It's inside the same structure as the accordion content\n",
    "                num_courses_text_element = wait.until(EC.presence_of_element_located(\n",
    "                    (By.XPATH, \"//div[@id='learningPlanSectionSECTION-B']//div[contains(@class, 'LearningPlanItems_description__')]/span[contains(text(), 'Complete')]\")\n",
    "                ))\n",
    "                scraped_data['total_courses_required_text'] = num_courses_text_element.text.strip()\n",
    "                print(f\"Scraped Total Courses Text: {scraped_data['total_courses_required_text']}\")\n",
    "            except TimeoutException:\n",
    "                print(\"Timeout: Could not find 'Complete X required' text after accordion expansion/check.\")\n",
    "                scraped_data['total_courses_required_text'] = \"N/A\"\n",
    "            except Exception as e:\n",
    "                print(f\"Error finding 'Complete X required' text: {e}\")\n",
    "                scraped_data['total_courses_required_text'] = \"N/A\"\n",
    "\n",
    "            try:\n",
    "                print(\"Waiting for course cards to be visible...\")\n",
    "                # Use a more specific locator for the cards within the section\n",
    "                course_card_containers = wait.until(EC.visibility_of_all_elements_located(\n",
    "                    (By.XPATH, \"//div[@id='learningPlanSectionSECTION-B']//div[contains(@class, 'ItemCard_itemCardContainer__')]\")\n",
    "                ))\n",
    "                print(f\"Found {len(course_card_containers)} course elements.\")\n",
    "\n",
    "                for j, card_container in enumerate(course_card_containers):\n",
    "                    course_info = {}\n",
    "                    course_info['parent_course_url'] = url # Link course to its parent URL\n",
    "                    course_info['parent_course_title'] = scraped_data.get('learning_plan_title', 'N/A')\n",
    "                    # ... (rest of your course scraping logic for name, duration, learners, rating) ...\n",
    "                    try:\n",
    "                        course_info['name'] = card_container.find_element(By.XPATH, \".//div[contains(@class, 'ItemCardComponents_searchTitle__')]/span\").text.strip()\n",
    "                    except NoSuchElementException:\n",
    "                        course_info['name'] = \"N/A\"\n",
    "                    \n",
    "                    try:\n",
    "                        duration_element = card_container.find_element(By.XPATH, \".//div[contains(@class, 'ActivityDuration_activityDuration__')]\")\n",
    "                        duration_text = duration_element.text.strip()\n",
    "                        course_info['duration'] = re.sub(r'Duration is\\s*', '', duration_text, flags=re.IGNORECASE).strip()\n",
    "                    except NoSuchElementException:\n",
    "                        course_info['duration'] = \"N/A\"\n",
    "\n",
    "                    try:\n",
    "                        learners_amount_div = card_container.find_element(By.XPATH, \".//div[contains(@class, 'LearnersAmount_learnersAmount__')]\")\n",
    "                        learners_text_raw = learners_amount_div.get_attribute('title')\n",
    "                        course_info['learners_text'] = learners_text_raw.strip() if learners_text_raw else \"N/A\"\n",
    "                        learners_count_span = learners_amount_div.find_element(By.XPATH, \".//span[@aria-hidden='true']\")\n",
    "                        course_info['learners_numeric'] = learners_count_span.text.strip().replace('.', '').replace(',', '')\n",
    "                    except (NoSuchElementException, StaleElementReferenceException):\n",
    "                        print(f\"Warning: Could not scrape learners for course {j+1} in {url}. Skipping or setting N/A.\")\n",
    "                        course_info['learners_text'] = \"N/A\"\n",
    "                        course_info['learners_numeric'] = \"N/A\"\n",
    "\n",
    "                    try:\n",
    "                        rating_element_div = card_container.find_element(By.XPATH, \".//div[contains(@class, 'Stars_starRating__')]\")\n",
    "                        rating_text_raw = rating_element_div.get_attribute('title')\n",
    "                        course_info['rating_text'] = rating_text_raw.strip() if rating_text_raw else \"N/A\"\n",
    "                        num_ratings_span = rating_element_div.find_element(By.XPATH, \".//span[contains(@class, 'Stars_numRatings__')]\")\n",
    "                        course_info['num_learners_rated_course'] = num_ratings_span.text.strip()\n",
    "                        rating_match = re.search(r'Average rating of ([\\d\\.]+) stars', course_info['rating_text'])\n",
    "                        if rating_match:\n",
    "                            course_info['average_rating_course'] = rating_match.group(1)\n",
    "                        else:\n",
    "                            course_info['average_rating_course'] = \"N/A\"\n",
    "                    except (NoSuchElementException, StaleElementReferenceException):\n",
    "                        print(f\"Warning: Could not scrape rating for course {j+1} in {url}. Skipping or setting N/A.\")\n",
    "                        course_info['rating_text'] = \"N/A\"\n",
    "                        course_info['num_learners_rated_course'] = \"N/A\"\n",
    "                        course_info['average_rating_course'] = \"N/A\"\n",
    "                        \n",
    "                    current_course_list.append(course_info)\n",
    "                    print(f\"  - Course {j+1}: Name='{course_info['name']}', Duration='{course_info['duration']}'...\")\n",
    "            except TimeoutException:\n",
    "                print(\"Timeout: No course cards found after accordion expansion/check.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error iterating and scraping individual course cards: {e}\")\n",
    "        else:\n",
    "            print(\"Skipping course card scraping as the parent section was not found.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during course section handling for {url}: {e}\")\n",
    "    \n",
    "    scraped_data['courses'] = current_course_list # Assign current courses to scraped_data\n",
    "\n",
    "    # 5. Scrape Tags\n",
    "    current_tags = [] # Store tags for the current URL\n",
    "    try:\n",
    "        print(\"\\n--- Attempting to scrape Tags section ---\")\n",
    "        tags_section_header = wait.until(EC.presence_of_element_located(\n",
    "            (By.XPATH, \"//div[contains(@class, 'TagGroup_tagGroupContainer__')]\")\n",
    "        ))\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView(true);\", tags_section_header)\n",
    "        print(\"Scrolled Tags section into view.\")\n",
    "        time.sleep(2)\n",
    "\n",
    "        tag_elements = driver.find_elements(By.XPATH, \"//div[contains(@class, 'TagGroup_tagGroupContainer__')]//div[contains(@class, 'TagLabel_labelContainer__')]/span[not(contains(@class, 'sr-only')) and not(contains(@class, 'TagLabel_chevronIcon__'))]\")\n",
    "        \n",
    "        for tag_el in tag_elements:\n",
    "            tag_text = tag_el.text.strip()\n",
    "            if tag_text:\n",
    "                current_tags.append(tag_text)\n",
    "        print(f\"Scraped Tags: {current_tags}\")\n",
    "    except NoSuchElementException:\n",
    "        print(\"Could not find Tags section or individual tags.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping tags for {url}: {e}\")\n",
    "    scraped_data['tags'] = current_tags # Assign current tags to scraped_data\n",
    "\n",
    "\n",
    "    # 6. Scrape Overall Rating & Reviews\n",
    "    scraped_data['overall_rating_reviews_section'] = \"N/A\"\n",
    "    scraped_data['num_learners_rated_reviews_section'] = \"N/A\"\n",
    "    current_rating_breakdown = {} # Store breakdown for the current URL\n",
    "    current_comments = [] # Store comments for the current URL\n",
    "\n",
    "    try:\n",
    "        print(\"\\n--- Attempting to scrape Ratings & Reviews section ---\")\n",
    "        ratings_header_element = wait.until(EC.presence_of_element_located(\n",
    "            (By.XPATH, \"//h2[contains(@class, 'RatingsAndComments_title__')]\")\n",
    "        ))\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView(true);\", ratings_header_element)\n",
    "        print(\"Scrolled Ratings & Reviews section into view.\")\n",
    "        time.sleep(2)\n",
    "\n",
    "        try:\n",
    "            overall_rating_text_element = wait.until(EC.presence_of_element_located(\n",
    "                (By.XPATH, \"//div[contains(@class, 'RatingSummary_description__Dthqd')]/span\")\n",
    "            ))\n",
    "            scraped_data['overall_rating_reviews_section'] = overall_rating_text_element.text.strip()\n",
    "            rating_match = re.search(r'([\\d\\.]+) out of 5', scraped_data['overall_rating_reviews_section'])\n",
    "            if rating_match:\n",
    "                scraped_data['overall_average_rating_reviews_section'] = rating_match.group(1)\n",
    "            else:\n",
    "                scraped_data['overall_average_rating_reviews_section'] = \"N/A\"\n",
    "            print(f\"Scraped Overall Average Rating: {scraped_data['overall_average_rating_reviews_section']}\")\n",
    "\n",
    "        except NoSuchElementException:\n",
    "            print(\"Could not find overall 'X out of 5' rating text.\")\n",
    "            scraped_data['overall_rating_reviews_section'] = \"N/A\"\n",
    "            scraped_data['overall_average_rating_reviews_section'] = \"N/A\"\n",
    "\n",
    "        try:\n",
    "            num_learners_rated_element = wait.until(EC.presence_of_element_located(\n",
    "                (By.XPATH, \"//div[contains(@class, 'RatingSummary_averageRating__8z9lv')]/span\")\n",
    "            ))\n",
    "            scraped_data['num_learners_rated_reviews_section'] = num_learners_rated_element.text.strip()\n",
    "            print(f\"Scraped Number of Learners Rated: {scraped_data['num_learners_rated_reviews_section']}\")\n",
    "        except NoSuchElementException:\n",
    "            print(\"Could not find 'Average rating by X learners' text.\")\n",
    "            scraped_data['num_learners_rated_reviews_section'] = \"N/A\"\n",
    "\n",
    "        rating_breakdown_bars = driver.find_elements(By.XPATH, \"//div[contains(@class, 'RatingBreakDown_container__UuLvg')]\")\n",
    "        if rating_breakdown_bars:\n",
    "            for bar in rating_breakdown_bars:\n",
    "                try:\n",
    "                    star_rating_div = bar.find_element(By.XPATH, \".//div[contains(@class, 'RatingBreakDown_starsAndTitle__EL5nE')]//div[contains(@class, 'Stars_starRating__')]\")\n",
    "                    star_count = star_rating_div.get_attribute('title')\n",
    "                    review_count_element = bar.find_element(By.XPATH, \".//div[contains(@class, 'RatingBreakDown_starsAndTitle__EL5nE')]/span[not(contains(@class, 'Stars_starRating__'))]\")\n",
    "                    review_count = review_count_element.text.strip() if review_count_element.text else \"N/A reviews\"\n",
    "                    if star_count and review_count:\n",
    "                        current_rating_breakdown[star_count] = review_count\n",
    "                except NoSuchElementException:\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    print(f\"Error scraping a specific rating bar: {e}. Skipping this bar.\")\n",
    "                    continue\n",
    "        print(f\"Scraped Rating Breakdown: {current_rating_breakdown}\")\n",
    "        scraped_data['rating_breakdown'] = current_rating_breakdown\n",
    "\n",
    "        # --- SCRAPING INDIVIDUAL COMMENTS ---\n",
    "        print(\"\\n--- Attempting to scrape individual comments ---\")\n",
    "        \n",
    "        try:\n",
    "            comments_section_parent = wait.until(EC.presence_of_element_located(\n",
    "                (By.XPATH, \"//div[@id='YL_ratings_and_comments_component']\")\n",
    "            ))\n",
    "            comments_list_container = comments_section_parent.find_element(By.XPATH, \".//div[contains(@class, 'Comments_commentsContainer__')]\")\n",
    "            print(\"Found comments list container.\")\n",
    "\n",
    "            while True:\n",
    "                try:\n",
    "                    load_more_button_locator = (By.XPATH, \"//button[contains(text(), 'Load more comments')]\")\n",
    "                    load_more_button = wait.until(EC.element_to_be_clickable(load_more_button_locator))\n",
    "                    driver.execute_script(\"arguments[0].scrollIntoView(true);\", load_more_button)\n",
    "                    print(\"Clicking 'Load more comments'...\")\n",
    "                    load_more_button.click()\n",
    "                    time.sleep(3)\n",
    "                except TimeoutException:\n",
    "                    print(\"No more 'Load more comments' button found or all comments loaded.\")\n",
    "                    break\n",
    "                except StaleElementReferenceException:\n",
    "                    print(\"Stale 'Load more comments' button, retrying...\")\n",
    "                    time.sleep(1)\n",
    "                    continue\n",
    "\n",
    "            comment_elements_locator = (By.XPATH, \"//div[contains(@class, 'Comment_commentContainer__caKxU')]\")\n",
    "            all_comments_on_page = wait.until(EC.presence_of_all_elements_located(comment_elements_locator))\n",
    "            print(f\"Found {len(all_comments_on_page)} individual comments for {url}.\")\n",
    "\n",
    "            for k, comment_el in enumerate(all_comments_on_page):\n",
    "                comment_data = {}\n",
    "                comment_data['parent_course_url'] = url # Link comment to its parent URL\n",
    "                comment_data['parent_course_title'] = scraped_data.get('learning_plan_title', 'N/A')\n",
    "                try:\n",
    "                    comment_data['reviewer_name'] = comment_el.find_element(By.XPATH, \".//div[contains(@class, 'ProfileIcon_container__NIYfG')]//img\").get_attribute('alt').strip()\n",
    "                except NoSuchElementException:\n",
    "                    comment_data['reviewer_name'] = \"N/A\"\n",
    "\n",
    "                try:\n",
    "                    comment_data['review_date'] = comment_el.find_element(By.XPATH, \".//div[contains(@class, 'Comment_date__ZCDZK')]\").text.strip()\n",
    "                except NoSuchElementException:\n",
    "                    comment_data['review_date'] = \"N/A\"\n",
    "                \n",
    "                try:\n",
    "                    comment_data['comment_title'] = comment_el.find_element(By.XPATH, \".//h3[contains(@class, 'Comment_title__')]\").text.strip()\n",
    "                except NoSuchElementException:\n",
    "                    comment_data['comment_title'] = \"N/A\"\n",
    "\n",
    "                try:\n",
    "                    comment_data['comment_text'] = comment_el.find_element(By.XPATH, \".//div[contains(@class, 'Comment_content__uYufg')]\").text.strip()\n",
    "                except NoSuchElementException:\n",
    "                    comment_data['comment_text'] = \"N/A\"\n",
    "\n",
    "                try:\n",
    "                    comment_rating_sr_only = comment_el.find_element(By.XPATH, \".//div[contains(@class, 'Stars_starRating__')]//span[contains(@class, 'sr-only')]\").text.strip()\n",
    "                    rating_match = re.search(r'rated ([\\d\\.]+) out of 5 stars', comment_rating_sr_only)\n",
    "                    if rating_match:\n",
    "                        comment_data['comment_rating'] = rating_match.group(1)\n",
    "                    else:\n",
    "                        comment_data['comment_rating'] = comment_rating_sr_only\n",
    "                except NoSuchElementException:\n",
    "                    comment_data['comment_rating'] = \"N/A\"\n",
    "\n",
    "                current_comments.append(comment_data)\n",
    "                print(f\"  - Comment {k+1} by {comment_data['reviewer_name']}: Rating='{comment_data['comment_rating']}', Date='{comment_data['review_date']}'...\")\n",
    "\n",
    "        except TimeoutException:\n",
    "            print(\"Timeout: Could not find the comments section or individual comment elements.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred during individual comment scraping for {url}: {e}\")\n",
    "        scraped_data['comments'] = current_comments # Assign current comments to scraped_data\n",
    "\n",
    "    except NoSuchElementException:\n",
    "        print(\"Could not find main Ratings & Reviews section parent.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during overall ratings and reviews section processing for {url}: {e}\")\n",
    "    \n",
    "    # Append the scraped data for the current URL to the overall lists\n",
    "    # General Info\n",
    "    all_general_info_data.append({\n",
    "        'Course URL': scraped_data.get('course_url'),\n",
    "        'Learning Plan Title': scraped_data.get('learning_plan_title'),\n",
    "        'Overall Duration': scraped_data.get('overall_duration'),\n",
    "        'Total Learners (Numeric)': scraped_data.get('overall_learners_amount_numeric'),\n",
    "        'Total Learners (Text)': scraped_data.get('overall_learners_amount_text'),\n",
    "        'Overall Average Rating (Header)': scraped_data.get('overall_average_rating_from_header'),\n",
    "        'Overall Learners Rated (Header)': scraped_data.get('overall_learners_rated_from_header'),\n",
    "        'About Learning Plan': scraped_data.get('about_learning_plan'),\n",
    "        'Total Courses Required Text': scraped_data.get('total_courses_required_text'),\n",
    "        'Overall Rating (Reviews Section)': scraped_data.get('overall_average_rating_reviews_section'),\n",
    "        'Number of Learners Rated (Reviews Section)': scraped_data.get('num_learners_rated_reviews_section'),\n",
    "        'Tags': \", \".join(scraped_data.get('tags', []))\n",
    "    })\n",
    "\n",
    "    # Courses (each course within a plan is a row)\n",
    "    for course_item in scraped_data['courses']:\n",
    "        all_courses_data.append(course_item)\n",
    "\n",
    "    # Comments (each comment is a row)\n",
    "    for comment_item in scraped_data['comments']:\n",
    "        all_comments_data.append(comment_item)\n",
    "\n",
    "    # Rating Breakdown (each breakdown item is a row, linked to the course URL)\n",
    "    if scraped_data['rating_breakdown']:\n",
    "        for stars, count in scraped_data['rating_breakdown'].items():\n",
    "            all_rating_breakdown_data.append({\n",
    "                'Course URL': scraped_data.get('course_url'),\n",
    "                'Learning Plan Title': scraped_data.get('learning_plan_title'),\n",
    "                'Stars': stars,\n",
    "                'Review Count': count\n",
    "            })\n",
    "\n",
    "    print(f\"\\n--- Finished scraping for {scraped_data.get('learning_plan_title', url)} ---\")\n",
    "\n",
    "\n",
    "# --- Convert to DataFrame and Save All Collected Data ---\n",
    "print(\"\\n--- Consolidating and Saving All Scraped Data ---\")\n",
    "\n",
    "# 1. Create a single DataFrame for general learning plan info from all URLs\n",
    "if all_general_info_data:\n",
    "    final_general_info_df = pd.DataFrame(all_general_info_data)\n",
    "    final_general_info_df.to_csv('ds_general_info.csv', index=False)\n",
    "    print(\"\\nSaved 'ds_general_info.csv'\")\n",
    "else:\n",
    "    print(\"No general info data to save.\")\n",
    "\n",
    "# 2. Create a single DataFrame for Courses from all URLs\n",
    "if all_courses_data:\n",
    "    final_courses_df = pd.DataFrame(all_courses_data)\n",
    "    final_courses_df.to_csv('ds_courses.csv', index=False)\n",
    "    print(\"Saved 'ds_courses.csv'\")\n",
    "else:\n",
    "    print(\"No course data to save.\")\n",
    "\n",
    "# 3. Create a single DataFrame for Comments from all URLs\n",
    "if all_comments_data:\n",
    "    final_comments_df = pd.DataFrame(all_comments_data)\n",
    "    final_comments_df.to_csv('ds_comments.csv', index=False)\n",
    "    print(\"Saved 'ds_comments.csv'\")\n",
    "else:\n",
    "    print(\"No comment data to save.\")\n",
    "\n",
    "# Keep the browser open for a few seconds, then close\n",
    "time.sleep(10)\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e1abdf3-2a76-49ec-877e-1e481f2d0df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting data cleaning process ---\n",
      "\n",
      "Cleaning 'ds_general_info.csv'...\n",
      "  - Applied 'Overall Duration' cleaning (extracted numeric part).\n",
      "  - Deleted columns from general info: ['Total Learners (Text)', 'Overall Rating (Reviews Section)', 'Number of Learners Rated (Reviews Section)', 'Total Courses Required Text', 'Overall Duration']\n",
      "  - Saved 'ds_general_info_cleaned.csv'\n",
      "\n",
      "Cleaning 'ds_courses.csv'...\n",
      "  - Deleted columns from courses: ['Learners_text', 'rating_text']\n",
      "  - Saved 'ds_courses_cleaned.csv'\n",
      "\n",
      "Cleaning 'ds_comments.csv'...\n",
      "  - Deleted columns from comments: ['comment_title']\n",
      "  - Saved 'ds_comments_cleaned.csv'\n",
      "\n",
      "--- Data cleaning process complete ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re # Import the regular expression module\n",
    "\n",
    "print(\"--- Starting data cleaning process ---\")\n",
    "\n",
    "# --- 1. Clean ds_general_info.csv ---\n",
    "try:\n",
    "    df_general_info = pd.read_csv('ds_general_info.csv')\n",
    "    print(\"\\nCleaning 'ds_general_info.csv'...\")\n",
    "\n",
    "    # Overall duration – extract only the number (e.g., 10 from \"About 10 hours\")\n",
    "    def extract_numeric_duration(duration_str):\n",
    "        if pd.isna(duration_str):\n",
    "            return None\n",
    "        # Use regex to find one or more digits (\\d+) in the string\n",
    "        match = re.search(r'(\\d+)', str(duration_str))\n",
    "        if match:\n",
    "            return float(match.group(1)) # Convert the found number to float\n",
    "        return None # Return None if no number is found\n",
    "\n",
    "    df_general_info['Overall Duration (in hours)'] = df_general_info['Overall Duration'].apply(extract_numeric_duration)\n",
    "    print(\"  - Applied 'Overall Duration' cleaning (extracted numeric part).\")\n",
    "\n",
    "\n",
    "    # Delete columns: total learners(text), overall rating (reviews section), Number of Learners Rated (Reviews Section)\n",
    "    # AND now also 'Total Courses Required Text' and the original 'Overall Duration'\n",
    "    columns_to_delete_general = [\n",
    "        'Total Learners (Text)',\n",
    "        'Overall Rating (Reviews Section)',\n",
    "        'Number of Learners Rated (Reviews Section)',\n",
    "        'Total Courses Required Text', # Added for deletion\n",
    "        'Overall Duration' # Added for deletion to replace with 'Overall Duration (Cleaned)'\n",
    "    ]\n",
    "    df_general_info.drop(columns=columns_to_delete_general, errors='ignore', inplace=True)\n",
    "    print(f\"  - Deleted columns from general info: {columns_to_delete_general}\")\n",
    "\n",
    "    # Save cleaned general info\n",
    "    df_general_info.to_csv('ds_general_info_cleaned.csv', index=False)\n",
    "    print(\"  - Saved 'ds_general_info_cleaned.csv'\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'ds_general_info.csv' not found. Skipping cleaning for this file.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while cleaning 'ds_general_info.csv': {e}\")\n",
    "\n",
    "\n",
    "# --- 2. Clean ds_courses.csv ---\n",
    "try:\n",
    "    df_courses = pd.read_csv('ds_courses.csv')\n",
    "    print(\"\\nCleaning 'ds_courses.csv'...\")\n",
    "\n",
    "    # Delete columns: Learners_text, rating_text\n",
    "    columns_to_delete_courses = [\n",
    "        'Learners_text',\n",
    "        'rating_text'\n",
    "    ]\n",
    "    df_courses.drop(columns=columns_to_delete_courses, errors='ignore', inplace=True)\n",
    "    print(f\"  - Deleted columns from courses: {columns_to_delete_courses}\")\n",
    "\n",
    "    # Save cleaned courses\n",
    "    df_courses.to_csv('ds_courses_cleaned.csv', index=False)\n",
    "    print(\"  - Saved 'ds_courses_cleaned.csv'\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'ds_courses.csv' not found. Skipping cleaning for this file.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while cleaning 'ds_courses.csv': {e}\")\n",
    "\n",
    "\n",
    "# --- 3. Clean ds_comments.csv ---\n",
    "try:\n",
    "    df_comments = pd.read_csv('ds_comments.csv')\n",
    "    print(\"\\nCleaning 'ds_comments.csv'...\")\n",
    "\n",
    "    # Delete column comment_title\n",
    "    columns_to_delete_comments = [\n",
    "        'comment_title'\n",
    "    ]\n",
    "    df_comments.drop(columns=columns_to_delete_comments, errors='ignore', inplace=True)\n",
    "    print(f\"  - Deleted columns from comments: {columns_to_delete_comments}\")\n",
    "\n",
    "    # Save cleaned comments\n",
    "    df_comments.to_csv('ds_comments_cleaned.csv', index=False)\n",
    "    print(\"  - Saved 'ds_comments_cleaned.csv'\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'ds_comments.csv' not found. Skipping cleaning for this file.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while cleaning 'ds_comments.csv': {e}\")\n",
    "\n",
    "print(\"\\n--- Data cleaning process complete ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
